# -*- coding: utf-8 -*-
"""RegresiónLogística.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1ZjFbCazgEfZuN-ePk9NvadAobx6WEehL
"""

'''
Código de regresiones lineales robustas. En el método "covariance" se pueden
cambiar los coeficientes de correlación.
'''

###DATASET STROKE
import pandas as pd
import numpy as np
from numpy import nan as NA
from statistics import stdev
import numpy as np
from scipy.stats import spearmanr
from statsmodels import robust
from scipy.stats import kendalltau
from scipy import stats



#data = pd.read_csv('https://raw.githubusercontent.com/asepulvede/ModelacionSimulacion4/main/healthcare-dataset-stroke-data.csv')
data = pd.read_csv('https://raw.githubusercontent.com/asepulvede/ModelacionSimulacion4/main/healthcare-dataset-stroke-data.csv') #Dataset con outliers
data= data.fillna(method='ffill')
data= pd.get_dummies(data, columns= ["gender"])
data= pd.get_dummies(data, columns= ["ever_married"])
data= pd.get_dummies(data, columns= ["work_type"])
data= pd.get_dummies(data, columns= ["Residence_type"])
data= pd.get_dummies(data, columns= ["smoking_status"])
data = data.drop(columns = ['gender_Other' ])
data=data.drop(columns='id')

data = data.loc[:, (data != 0).any(axis=0)]

m = len(data.columns)

for i in range(m):
  if stdev(data.iloc[:,i])==0:
    data =data.drop(columns = [i])

##FEATURES STROKE

features=data.drop(columns='stroke')
target = data.iloc[:,2]

##MATRIZ DE COVARIANZA ROBUSTA MÉTODOS
def covariance(x,y):
  #coef, p = kendalltau(x,y)
  #coef, p = stats.pearsonr(x,y)
  coef, p = spearmanr(x,y)
  VI = coef*(stdev(x)*stdev(y)) #Existe la opción de hacerlo con el MAD para que sea más robusto,
  #Sin embargo, con nuestros datos esto resulta en una matriz singular (no invertible)
  return VI

def covarianceM(x):
  covar = np.zeros((len(x.columns),len(x.columns)), dtype=float)
  for i,j in np.ndindex(covar.shape): 
      VI=covariance(x.iloc[:,i],x.iloc[:,j])
      covar[i,j] = VI
  return covar

##BETAS 

def betas(features, target):
  cova = covarianceM(features)
  Cxinv = np.linalg.inv(cova)
  covars = np.zeros((m-1), dtype=float)
  for i in range(m-1):
    covars[i] = covariance(features.iloc[:,i],target)
  Betas=np.dot(Cxinv,(covars.T))

##BETA 0
  promedio = target.mean()
  Beta0 = promedio - np.dot(features.mean(axis=0),Betas)
  return Betas, Beta0

#Link function o función sigmoide
def sigmoid(betas, beta0, features):
    Xbetas = np.dot(features,betas)
    y = beta0 + Xbetas
    return 1 / (1 + np.exp(-y))

##Prueba
from sklearn.model_selection import train_test_split
import sklearn.metrics as metrics
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

auc = []
acc = []
for i in range(100):
  try:
    x_train, x_test, y_train, y_test=train_test_split(features,target)
    Betas, Beta0 = betas(x_train,y_train)
    Prob = sigmoid(Betas, Beta0, x_test) 
    Prediction = np.zeros((len(x_test)), dtype=float)
    for i in range(len(x_test)):
      if Prob[i]>=0.5:
        Prediction[i]=1.0
      else:
        Prediction[i]=0.0
    fpr, tpr, threshold = metrics.roc_curve(y_test,Prediction)
    roc_auc = metrics.auc(fpr, tpr)
    auc.append(roc_auc)
    acc.append(accuracy_score(y_test, Prediction))
  except Exception:
    pass
    
print(acc)
print(auc)

import matplotlib.pyplot as plt

plt.plot(acc)
plt.title('Acurracy Scores por iteración Spearman')
plt.xlabel('Iteraciones')
plt.ylabel('Puntaje')
plt.show()


plt.plot(auc, color= 'g')
plt.title('AUC por iteración Spearman')
plt.xlabel('Iteraciones')
plt.ylabel('AUC')
plt.show()
